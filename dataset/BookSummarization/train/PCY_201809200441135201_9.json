{
  "passage_id": "PCY_201809200441135201_9",
  "metadata": {
    "doc_id": "PCY_201809200441135201",
    "doc_type": "도서",
    "doc_name": "인공지능 대응에 있어 형사법 이론의 한계",
    "author": "이원상",
    "publisher": "대검찰청",
    "published_year": "2018",
    "kdc_label": "법무및검찰",
    "kdc_code": "300"
  },
  "chapter": null,
  "passage": "로봇 윤리(Robot Ethics) 분야에서는 크게 세 가지의 물음에 대한 답을 구하고있다. 첫째는 인간이 로봇에게 어떻게 윤리적으로 대할 수 있을지에 대한 것이고, 둘째는 로봇이 윤리적으로 행동을 하도록 하거나 로봇을 어떻게 하면 윤리적인 행위자로 만들 수 있을지에 대한 것이며, 셋째는 인간과 로봇은 윤리적인 관계를 맺을 수 있으며, 윤리적 행위자인 인간이 로봇에 대해 어떤 책임을 질 수 있는지에 관한 것이다. 첫 번째 물음은 로봇이 윤리적인 존재가 되었을 때 주로 문제가 될 수 있을 것이다. 두 번째 물음은 최근 자율주행 자동차의 상용이 눈앞에 왔고, 인공지능 에이전트인 왓슨이 의료에 투입되며, 전쟁에 인공지능 로봇의 활용이 적극적으로 추진되는 등 인공지능 에이전트나 로봇의 활용성이 높아지면서 활발히 논의되고 있다. 그러므로 로봇을 무도덕적(amoral) 존재에서 도덕적(moral) 존재로 만들기 위한 노력들이 계속되고 있다. 이는 로봇에게 단순한 인간의 윤리를 주입시키는 것이 아니라 로봇이 윤리 추론 능력을 갖추게 되어 윤리를 학습하고, 도덕성을 개발하며, 스스로의 윤리 체계를 진화시키는 것을 의미하는 동적인 도덕지능(dynamic moral intelligence)을 지닌 존재로 만드는 것이다. 그러나 인공지능 에이전트나 로봇이 그 정도의 수준에 이르는 것은 어쩌면 좀 더 먼 미래의 이야기일수 있다. 따라서 최근 논의들은 윤리적인 인간이 아직은 도덕적이거나 윤리적이지 않은 로봇에 대해 어떤 책임을 질 수 있을지에 대한 세 번째 물음에 대한 논의가 좀 더 활발히 진행되고 있다. 그러면서 윤리적인 로봇의 설계자나 제조사, 관리자, 그리고 사용자가 인공지능 에이전트나 로봇을 통해 발생하는 문제들에 대해 법적인 책임을 질 수 있도록 법적 책임 관점에서 접근하는 것이 보다 현실적이라고 한다.",
  "summary": "로봇 윤리분야에서는 세 가지 답을 구하는데, 첫 번째 물음은 사람이 로봇을 윤리적으로 대할 수 있을지로,  이는 로봇이 윤리적인 존재여야 문제가 될 수 있을 것이다. 두 번째 물음은 로봇이 어떻게 윤리적 행위자가 될수 있는지를 묻는데 로봇을 동적인 도덕지능을 지닌 존재로 만드는 것이다. 세 번째 물음은 윤리적인 인간이 윤리적이지 않은 로봇에 대한 어떤 책임을 질 수 있을지에 대한 물음으로, 이에 대한 논의가 활발히 진행중인데 법적 책임측면으로 접근하는 것이 현실적이다.  "
}