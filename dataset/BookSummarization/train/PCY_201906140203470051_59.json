{
  "passage_id": "PCY_201906140203470051_59",
  "metadata": {
    "doc_id": "PCY_201906140203470051",
    "doc_type": "도서",
    "doc_name": "4차 산업혁명 시대에서 정보인권 보호를 위한 실태조사",
    "author": null,
    "publisher": "국가인권위원회",
    "published_year": "2018",
    "kdc_label": "산업진흥·고도화",
    "kdc_code": "300"
  },
  "chapter": null,
  "passage": "우선 인공지능 기술이 어떤 결정을 내렸다고 할 때, 그것이 어떠한 과정이나 논리에 따라 그러한 결정이 내려졌는지 알기 어렵다는 점이 문제다. 일명 블랙박스와 같은 속성을 지니고 있기 때문에 내부의 알고리즘이 알려지지 않은 채 부적절한 결정이 내려질 수 있다. 또한 소스가 되는 빅데이터 혹은 그것을 분석하는 사람과 플랫폼에 내재된 편향 때문에 인종, 민족, 성별, 사회적 계층 등에 대해 편향된 결과를 내놓을 수 있다. 이로써 특정한 인구통계학적 특성을 가진 사람들은 인공지능에 의해 불합리하게 차별받거나 배제될 가능성이 높다. 예컨대 인공지능은 현 상태를 평가하고 미래를 예측하는 결정을 내리는 영역에서 광범위하게 사용될 것인데, 그 미래 예측의 알고리즘은 인간이 배제되기 때문에 공정한 것이 아니라 오히려 인간적 편견에 사로잡혀 옳지 못한 판단을 내리게 될 위험성이 다분하다.",
  "summary": "인공지능 기술이 내린 결정이 어떤 과정이나 논리에 따라 결정한 것인지 알 수 없다. 미래를 예측하는 결정을 내리는 영역에서의 알고리즘은 인간이 배제되기 때문에 공정하지 않으며 잘못된 판단을 내리게 될 위험이 있다."
}